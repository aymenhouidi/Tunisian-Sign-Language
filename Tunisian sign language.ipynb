{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Tunisian Sign Language EDA","metadata":{}},{"cell_type":"markdown","source":"## Problematic :","metadata":{}},{"cell_type":"markdown","source":"The situation for deaf people in Tunisia is challenging, with estimates of\nthe deaf population ranging from 40,000 to 60,000 people. These individuals\nface significant difficulties communicating with non-deaf individuals, as many\npeople do not understand or know sign language. As a result, deaf individuals\noften find themselves in situations where verbal communication is the norm,\nwhich can lead to feelings of isolation and exclusion from society.\nAdditionally, the lack of access to expert interpretation services in many circumstances\ncan exacerbate these challenges. This can lead to underemployment,\npublic health issues, and other difficulties that can create a definitive\ngate between deaf individuals and society.\nUnfortunately, there are currently no official","metadata":{}},{"cell_type":"markdown","source":"## Proposed solution","metadata":{"execution":{"iopub.status.busy":"2023-05-11T15:27:03.948065Z","iopub.execute_input":"2023-05-11T15:27:03.948467Z","iopub.status.idle":"2023-05-11T15:27:03.954850Z","shell.execute_reply.started":"2023-05-11T15:27:03.948431Z","shell.execute_reply":"2023-05-11T15:27:03.953694Z"}}},{"cell_type":"markdown","source":"**Our proposed solution involves developing a deep learning model for Tunisian\nSign Language recognition and integrating it into a mobile application using\nTensorFlow Lite.**","metadata":{}},{"cell_type":"markdown","source":"This works was inspired by this two notebooks : \n* https://www.kaggle.com/code/danielpeshkov/animated-data-visualization\n* https://www.kaggle.com/code/dschettler8845/gislr-learn-eda-baseline","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:18:54.490014Z","iopub.execute_input":"2023-05-11T16:18:54.490397Z","iopub.status.idle":"2023-05-11T16:18:54.497620Z","shell.execute_reply.started":"2023-05-11T16:18:54.490364Z","shell.execute_reply":"2023-05-11T16:18:54.496156Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n%matplotlib inline\nimport matplotlib.pyplot as plt # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split, GroupShuffleSplit \n\nimport glob\nimport sys\nimport os\nimport math\nimport gc\nimport sys\nimport sklearn\nimport scipy\nimport json\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-16T21:07:35.520847Z","iopub.execute_input":"2023-05-16T21:07:35.521271Z","iopub.status.idle":"2023-05-16T21:07:39.507099Z","shell.execute_reply.started":"2023-05-16T21:07:35.521236Z","shell.execute_reply":"2023-05-16T21:07:39.506012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA & Visualization ","metadata":{}},{"cell_type":"code","source":"# Load the train dataframe\ntrain = pd.read_csv('/kaggle/input/tunisian-sign-language-landmarks/train.csv')\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.508523Z","iopub.execute_input":"2023-05-16T21:07:39.509261Z","iopub.status.idle":"2023-05-16T21:07:39.522384Z","shell.execute_reply.started":"2023-05-16T21:07:39.509223Z","shell.execute_reply":"2023-05-16T21:07:39.520949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change the path column\ntrain['path'] = train['path'].str.replace(r'output_dir\\\\', '/kaggle/input/tunisian-sign-language-landmarks/output_dir/')\ntrain.loc[train[\"sign\"]==\"tleth\", \"sign\"] = \"thleth\"\ntrain.loc[train[\"sign\"]==\"inty\", \"sign\"] = \"Inty\"","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.523851Z","iopub.execute_input":"2023-05-16T21:07:39.525321Z","iopub.status.idle":"2023-05-16T21:07:39.536235Z","shell.execute_reply.started":"2023-05-16T21:07:39.525280Z","shell.execute_reply":"2023-05-16T21:07:39.534801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.540225Z","iopub.execute_input":"2023-05-16T21:07:39.540675Z","iopub.status.idle":"2023-05-16T21:07:39.562750Z","shell.execute_reply.started":"2023-05-16T21:07:39.540636Z","shell.execute_reply":"2023-05-16T21:07:39.561396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the number of categories\nnum_signs = len(train['sign'].unique())\nnum_signs","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.564282Z","iopub.execute_input":"2023-05-16T21:07:39.564658Z","iopub.status.idle":"2023-05-16T21:07:39.571818Z","shell.execute_reply.started":"2023-05-16T21:07:39.564625Z","shell.execute_reply":"2023-05-16T21:07:39.570865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data set includes 11 signs and covers the primary terms used in vital domains of the deaf community in Tunisia. These signs were classified into five categories: \n* Demands: Commonly used terms such as Hello, How are you, Help me .. \n* Famille: Family Words such as mom, dad, aunt .. \n* Destinations: Most popular destinations like hospital,municipality\n* Jours: Weekdays.\n* Transportation: Modes of transportation such as vehicle and bus... ","metadata":{"execution":{"iopub.status.busy":"2023-05-11T15:35:38.518270Z","iopub.execute_input":"2023-05-11T15:35:38.518642Z","iopub.status.idle":"2023-05-11T15:35:38.527474Z","shell.execute_reply.started":"2023-05-11T15:35:38.518590Z","shell.execute_reply":"2023-05-11T15:35:38.526081Z"}}},{"cell_type":"code","source":"# plt.figure(figsize=(10, 6))\n# sns.countplot(data = train, y = 'Categorie', order = train.Categorie.value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.572800Z","iopub.execute_input":"2023-05-16T21:07:39.573139Z","iopub.status.idle":"2023-05-16T21:07:39.580601Z","shell.execute_reply.started":"2023-05-16T21:07:39.573111Z","shell.execute_reply":"2023-05-16T21:07:39.579741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 8))\ntrain[\"sign\"].value_counts().head(25).sort_values(ascending=True).plot(\n    kind=\"barh\", ax=ax, title=\"Signs in Training Dataset\"\n)\nax.set_xlabel(\"Number of Training Examples\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.581675Z","iopub.execute_input":"2023-05-16T21:07:39.582132Z","iopub.status.idle":"2023-05-16T21:07:39.990567Z","shell.execute_reply.started":"2023-05-16T21:07:39.582087Z","shell.execute_reply":"2023-05-16T21:07:39.989462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequence landmarks data","metadata":{}},{"cell_type":"markdown","source":"Overall Nature of Data\nWe have x-y-z coordinates of landmark indices of both left and right hands for each frame of a sequence. We need to use all/some of the frames to classify the sequence as a whole into the 10 odd signs there are. Below are the landmark indices of hand from the mediapipe page.","metadata":{}},{"cell_type":"markdown","source":"![Mediapipe Hand landmarks](https://mediapipe.dev/images/mobile/hand_landmarks.png)","metadata":{}},{"cell_type":"code","source":"example_fn = train.query('sign == \"car\"')[\"path\"].values[0]\n\nexample_landmark = pd.read_parquet(f\"{example_fn}\")\nexample_landmark.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:39.992380Z","iopub.execute_input":"2023-05-16T21:07:39.993119Z","iopub.status.idle":"2023-05-16T21:07:40.115082Z","shell.execute_reply.started":"2023-05-16T21:07:39.993064Z","shell.execute_reply":"2023-05-16T21:07:40.114229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are x-y-z locations. We can perhaps combine all of the details of a frame in a single row when we move on to the modeling part. Here it gives us a little more detailed look into the data.","metadata":{}},{"cell_type":"code","source":"example_landmark.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:40.116334Z","iopub.execute_input":"2023-05-16T21:07:40.116852Z","iopub.status.idle":"2023-05-16T21:07:40.124149Z","shell.execute_reply.started":"2023-05-16T21:07:40.116822Z","shell.execute_reply":"2023-05-16T21:07:40.122980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape is 21 Landmark * 2 Hands * 38 frame = 1596","metadata":{}},{"cell_type":"code","source":"example_landmark.describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:40.125399Z","iopub.execute_input":"2023-05-16T21:07:40.126197Z","iopub.status.idle":"2023-05-16T21:07:40.166197Z","shell.execute_reply.started":"2023-05-16T21:07:40.126163Z","shell.execute_reply":"2023-05-16T21:07:40.165139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_frames = example_landmark[\"frame\"].nunique()\nunique_types = example_landmark[\"type\"].nunique()\n\nprint(\n    f\"The file has {unique_frames} unique frames and {unique_types} types of landmarks\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:40.168158Z","iopub.execute_input":"2023-05-16T21:07:40.168647Z","iopub.status.idle":"2023-05-16T21:07:40.177790Z","shell.execute_reply.started":"2023-05-16T21:07:40.168602Z","shell.execute_reply":"2023-05-16T21:07:40.176208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"listen_files = train.query('sign == \"cv\"')[\"path\"].values\nfor i, f in enumerate(listen_files):\n    example_landmark = pd.read_parquet(f\"{f}\")\n    unique_frames = example_landmark[\"frame\"].nunique()\n    unique_types = example_landmark[\"type\"].nunique()\n    types_in_video = example_landmark[\"type\"].unique()\n    print(\n        f\"The file has {unique_frames} unique frames and {unique_types} unique types: {types_in_video}\"\n    )\n    if i == 20:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:40.179894Z","iopub.execute_input":"2023-05-16T21:07:40.180399Z","iopub.status.idle":"2023-05-16T21:07:40.242106Z","shell.execute_reply.started":"2023-05-16T21:07:40.180355Z","shell.execute_reply":"2023-05-16T21:07:40.240917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_PARQUETS_TO_READ = 155  # So we don't have to load all 95k\n\ncombined_meta = {}\nfor i, d in tqdm(train.iterrows(), total=len(train)):\n    file_path = d[\"path\"]\n    example_landmark = pd.read_parquet(f\"{file_path}\")\n    # Get the number of landmarks with x,y,z data per type\n    meta = (\n        example_landmark.dropna(subset=[\"x\", \"y\", \"z\"])[\"type\"].value_counts().to_dict()\n    )\n    meta[\"frame\"] = example_landmark[\"frame\"].nunique()\n    xyz_meta = (\n        example_landmark.agg(\n            {\n                \"x\": [\"min\", \"max\", \"mean\"],\n                \"y\": [\"min\", \"max\", \"mean\"],\n                \"z\": [\"min\", \"max\", \"mean\"],\n            }\n        )\n        .unstack()\n        .to_dict()\n    )\n\n    for key in xyz_meta.keys():\n        new_key = key[0] + \"_\" + key[1]\n        meta[new_key] = xyz_meta[key]\n    combined_meta[file_path] = meta\n    if i >= N_PARQUETS_TO_READ:\n        break\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:40.247755Z","iopub.execute_input":"2023-05-16T21:07:40.248175Z","iopub.status.idle":"2023-05-16T21:07:41.271627Z","shell.execute_reply.started":"2023-05-16T21:07:40.248138Z","shell.execute_reply":"2023-05-16T21:07:41.270180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_with_meta = train.merge(\n    pd.DataFrame(combined_meta).T.reset_index().rename(columns={\"index\": \"path\"}),\n    how=\"left\",\n)\ntrain_with_meta.to_parquet(\"train_with_meta.parquet\")","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.273233Z","iopub.execute_input":"2023-05-16T21:07:41.274085Z","iopub.status.idle":"2023-05-16T21:07:41.299940Z","shell.execute_reply.started":"2023-05-16T21:07:41.274041Z","shell.execute_reply":"2023-05-16T21:07:41.298747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_with_meta.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.301541Z","iopub.execute_input":"2023-05-16T21:07:41.302611Z","iopub.status.idle":"2023-05-16T21:07:41.331359Z","shell.execute_reply.started":"2023-05-16T21:07:41.302573Z","shell.execute_reply":"2023-05-16T21:07:41.329763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_with_meta[[\"left_hand\", \"right_hand\",\"pose\"]].sum().sort_values().plot(\n    kind=\"barh\", title=\"Sum of Rows by Landmark Type\"\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.333515Z","iopub.execute_input":"2023-05-16T21:07:41.334237Z","iopub.status.idle":"2023-05-16T21:07:41.624437Z","shell.execute_reply.started":"2023-05-16T21:07:41.334190Z","shell.execute_reply":"2023-05-16T21:07:41.623137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data animation","metadata":{}},{"cell_type":"markdown","source":"This is the file we will be looking at. Feel free to change the directory to any of the files available to visualize them as well.","metadata":{}},{"cell_type":"code","source":"from matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-16T21:07:41.626456Z","iopub.execute_input":"2023-05-16T21:07:41.626966Z","iopub.status.idle":"2023-05-16T21:07:41.637199Z","shell.execute_reply.started":"2023-05-16T21:07:41.626898Z","shell.execute_reply":"2023-05-16T21:07:41.635732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Change this directory to any file\npath_to_sign = '/kaggle/input/tunisian-sign-language-landmarks/output_dir/005cv.parquet'\nsign = pd.read_parquet(f'{path_to_sign}')\nsign.y = sign.y * -1","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.640256Z","iopub.execute_input":"2023-05-16T21:07:41.641331Z","iopub.status.idle":"2023-05-16T21:07:41.657633Z","shell.execute_reply.started":"2023-05-16T21:07:41.641265Z","shell.execute_reply":"2023-05-16T21:07:41.656443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### sign.head(50)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T00:18:30.490676Z","iopub.execute_input":"2023-05-12T00:18:30.491190Z","iopub.status.idle":"2023-05-12T00:18:30.526412Z","shell.execute_reply.started":"2023-05-12T00:18:30.491147Z","shell.execute_reply":"2023-05-12T00:18:30.525177Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"sign.frame.unique()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.659183Z","iopub.execute_input":"2023-05-16T21:07:41.659664Z","iopub.status.idle":"2023-05-16T21:07:41.669916Z","shell.execute_reply.started":"2023-05-16T21:07:41.659620Z","shell.execute_reply":"2023-05-16T21:07:41.668717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_hand_points(hand):\n    x = [[hand.iloc[0].x, hand.iloc[1].x, hand.iloc[2].x, hand.iloc[3].x, hand.iloc[4].x], # Thumb\n         [hand.iloc[5].x, hand.iloc[6].x, hand.iloc[7].x, hand.iloc[8].x], # Index\n         [hand.iloc[9].x, hand.iloc[10].x, hand.iloc[11].x, hand.iloc[12].x], \n         [hand.iloc[13].x, hand.iloc[14].x, hand.iloc[15].x, hand.iloc[16].x], \n         [hand.iloc[17].x, hand.iloc[18].x, hand.iloc[19].x, hand.iloc[20].x], \n         [hand.iloc[0].x, hand.iloc[5].x, hand.iloc[9].x, hand.iloc[13].x, hand.iloc[17].x, hand.iloc[0].x]]\n\n    y = [[hand.iloc[0].y, hand.iloc[1].y, hand.iloc[2].y, hand.iloc[3].y, hand.iloc[4].y],  #Thumb\n         [hand.iloc[5].y, hand.iloc[6].y, hand.iloc[7].y, hand.iloc[8].y], # Index\n         [hand.iloc[9].y, hand.iloc[10].y, hand.iloc[11].y, hand.iloc[12].y], \n         [hand.iloc[13].y, hand.iloc[14].y, hand.iloc[15].y, hand.iloc[16].y], \n         [hand.iloc[17].y, hand.iloc[18].y, hand.iloc[19].y, hand.iloc[20].y], \n         [hand.iloc[0].y, hand.iloc[5].y, hand.iloc[9].y, hand.iloc[13].y, hand.iloc[17].y, hand.iloc[0].y]] \n    return x, y","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.672059Z","iopub.execute_input":"2023-05-16T21:07:41.672531Z","iopub.status.idle":"2023-05-16T21:07:41.690876Z","shell.execute_reply.started":"2023-05-16T21:07:41.672485Z","shell.execute_reply":"2023-05-16T21:07:41.689213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sign[\"type\"].unique()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.692871Z","iopub.execute_input":"2023-05-16T21:07:41.693652Z","iopub.status.idle":"2023-05-16T21:07:41.709331Z","shell.execute_reply.started":"2023-05-16T21:07:41.693603Z","shell.execute_reply":"2023-05-16T21:07:41.707790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sign = sign[sign.type=='left_hand'].dropna()\ndef animation_frame(f):\n    frame = sign[sign.frame==f]\n    left = frame[frame.type=='left_hand']\n    lx, ly = get_hand_points(left)\n    ax.clear()\n    for i in range(len(lx)):\n        ax.plot(lx[i], ly[i])\n    plt.xlim(xmin, xmax)\n    plt.ylim(ymin, ymax)\n\n        \nprint(f\"The sign being shown here is: {train[train.path==f'{path_to_sign}'].sign.values[0]}\")\n\n## These values set the limits on the graph to stabilize the video\nxmin = sign.x.min() - 0.2\nxmax = sign.x.max() + 0.2\nymin = sign.y.min() - 0.2\nymax = sign.y.max() + 0.2\n\nfig, ax = plt.subplots()\nl, = ax.plot([], [])\nanimation = FuncAnimation(fig, func=animation_frame, frames=sign.frame.unique())\n\nHTML(animation.to_html5_video())","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:41.711363Z","iopub.execute_input":"2023-05-16T21:07:41.712305Z","iopub.status.idle":"2023-05-16T21:07:57.738374Z","shell.execute_reply.started":"2023-05-16T21:07:41.712253Z","shell.execute_reply":"2023-05-16T21:07:57.737013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing and modeling","metadata":{}},{"cell_type":"code","source":"DATA_COLUMNS    = ['x', 'y', 'z']\nROWS_PER_FRAME  = 42\nNUM_SHARDS      = 2\nSAVE_PATH       = '/kaggle/working/'\nBATCH_SIZE = 32\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.740018Z","iopub.execute_input":"2023-05-16T21:07:57.740510Z","iopub.status.idle":"2023-05-16T21:07:57.746384Z","shell.execute_reply.started":"2023-05-16T21:07:57.740465Z","shell.execute_reply":"2023-05-16T21:07:57.745476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_relevant_data_subset(pq_path):\n    data = pd.read_parquet(pq_path, columns=DATA_COLUMNS)\n    n_rows = len(data)\n    if n_rows % ROWS_PER_FRAME != 0:\n        n_rows = (n_rows // ROWS_PER_FRAME) * ROWS_PER_FRAME\n        data = data.iloc[:n_rows]\n    n_frames = int(n_rows / ROWS_PER_FRAME)\n    data = data.values.astype(np.float32)\n    return data.reshape(n_frames, ROWS_PER_FRAME, len(DATA_COLUMNS))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.747460Z","iopub.execute_input":"2023-05-16T21:07:57.747799Z","iopub.status.idle":"2023-05-16T21:07:57.761275Z","shell.execute_reply.started":"2023-05-16T21:07:57.747769Z","shell.execute_reply":"2023-05-16T21:07:57.759883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_get_features(ftensor):\n    def feat_wrapper(ftensor):\n        return load_relevant_data_subset(ftensor.numpy().decode('utf-8'))\n    return tf.py_function(\n        feat_wrapper,\n        [ftensor],\n        Tout=tf.float32\n    )","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.762872Z","iopub.execute_input":"2023-05-16T21:07:57.763332Z","iopub.status.idle":"2023-05-16T21:07:57.779579Z","shell.execute_reply.started":"2023-05-16T21:07:57.763291Z","shell.execute_reply":"2023-05-16T21:07:57.778409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_shape(x):\n    \n    # None dimensions can be of any length\n    return tf.ensure_shape(x, (None, ROWS_PER_FRAME, len(DATA_COLUMNS)))","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.781551Z","iopub.execute_input":"2023-05-16T21:07:57.782024Z","iopub.status.idle":"2023-05-16T21:07:57.793644Z","shell.execute_reply.started":"2023-05-16T21:07:57.781983Z","shell.execute_reply":"2023-05-16T21:07:57.792520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.795535Z","iopub.execute_input":"2023-05-16T21:07:57.796170Z","iopub.status.idle":"2023-05-16T21:07:57.817223Z","shell.execute_reply.started":"2023-05-16T21:07:57.796132Z","shell.execute_reply":"2023-05-16T21:07:57.816310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the signs that contains plus than 3 samples in the train set\nsign_counts = train[\"sign\"].value_counts()\ntrain_filtered = train[train[\"sign\"].isin(sign_counts[sign_counts > 3].index)]\nprint(train_filtered[\"sign\"].nunique())","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.818958Z","iopub.execute_input":"2023-05-16T21:07:57.819517Z","iopub.status.idle":"2023-05-16T21:07:57.831630Z","shell.execute_reply.started":"2023-05-16T21:07:57.819483Z","shell.execute_reply":"2023-05-16T21:07:57.830496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add ordinally Encoded Sign (assign number to each sign name)\ntrain_filtered['sign_ord'] = train_filtered['sign'].astype('category').cat.codes\n\n# Dictionaries to translate sign <-> ordinal encoded sign\nSIGN2ORD = train_filtered[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.833209Z","iopub.execute_input":"2023-05-16T21:07:57.833571Z","iopub.status.idle":"2023-05-16T21:07:57.844664Z","shell.execute_reply.started":"2023-05-16T21:07:57.833540Z","shell.execute_reply":"2023-05-16T21:07:57.843414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIGN2ORD","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.846334Z","iopub.execute_input":"2023-05-16T21:07:57.846695Z","iopub.status.idle":"2023-05-16T21:07:57.859201Z","shell.execute_reply.started":"2023-05-16T21:07:57.846664Z","shell.execute_reply":"2023-05-16T21:07:57.857993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# assume X contains the input data and y contains the class labels\nX = train_filtered.drop(\"sign\",axis=1)\ny = train_filtered.sign\n\n# perform stratified train-validation split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.860992Z","iopub.execute_input":"2023-05-16T21:07:57.861357Z","iopub.status.idle":"2023-05-16T21:07:57.874907Z","shell.execute_reply.started":"2023-05-16T21:07:57.861324Z","shell.execute_reply":"2023-05-16T21:07:57.873921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the two DataFrames along the rows axis\ntrain_final = pd.concat([X_train, y_train], axis=1)\ntrain_final.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.876255Z","iopub.execute_input":"2023-05-16T21:07:57.876611Z","iopub.status.idle":"2023-05-16T21:07:57.897418Z","shell.execute_reply.started":"2023-05-16T21:07:57.876579Z","shell.execute_reply":"2023-05-16T21:07:57.896107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate the two DataFrames along the rows axis\nval = pd.concat([X_val, y_val], axis=1)\nval.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.898693Z","iopub.execute_input":"2023-05-16T21:07:57.899863Z","iopub.status.idle":"2023-05-16T21:07:57.918144Z","shell.execute_reply.started":"2023-05-16T21:07:57.899827Z","shell.execute_reply":"2023-05-16T21:07:57.917047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_ds = tf.data.Dataset.from_tensor_slices(\n    train_final.path.values                                              # start with a dataset of the parquet paths\n).map(\n    tf_get_features                                                   # load individual sequences\n).map(\n    set_shape                                                         # set and enforce element shape\n).apply(\n    tf.data.experimental.dense_to_ragged_batch(batch_size=BATCH_SIZE) # apply batching function\n)\n\n# load and batch the labels\ny_ds = tf.data.Dataset.from_tensor_slices(\n    train_final.sign.map(SIGN2ORD).values.reshape(-1,1)\n).batch(BATCH_SIZE)\n\n# zip the features and labels\ntrain_ds = tf.data.Dataset.zip((X_ds, y_ds))","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:57.919650Z","iopub.execute_input":"2023-05-16T21:07:57.920125Z","iopub.status.idle":"2023-05-16T21:07:58.122699Z","shell.execute_reply.started":"2023-05-16T21:07:57.920089Z","shell.execute_reply":"2023-05-16T21:07:58.121536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sharding could be improved, as the distribution of elements in different shards should optimally be equal.\n# Currently, it will be a sample from a uniform distribution because this is simple to implement\ndef shard_func(*_):\n    return tf.random.uniform(shape=[], maxval=NUM_SHARDS, dtype=tf.int64)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.124066Z","iopub.execute_input":"2023-05-16T21:07:58.124412Z","iopub.status.idle":"2023-05-16T21:07:58.130277Z","shell.execute_reply.started":"2023-05-16T21:07:58.124382Z","shell.execute_reply":"2023-05-16T21:07:58.129293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds.prefetch(tf.data.AUTOTUNE).save(SAVE_PATH, shard_func=shard_func)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.131430Z","iopub.execute_input":"2023-05-16T21:07:58.131993Z","iopub.status.idle":"2023-05-16T21:07:58.496358Z","shell.execute_reply.started":"2023-05-16T21:07:58.131947Z","shell.execute_reply":"2023-05-16T21:07:58.495276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_throughput(ds_path):\n    for x in tqdm(tf.data.Dataset.load(ds_path)):\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.498025Z","iopub.execute_input":"2023-05-16T21:07:58.498375Z","iopub.status.idle":"2023-05-16T21:07:58.503737Z","shell.execute_reply.started":"2023-05-16T21:07:58.498344Z","shell.execute_reply":"2023-05-16T21:07:58.502487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_throughput(SAVE_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.505106Z","iopub.execute_input":"2023-05-16T21:07:58.505504Z","iopub.status.idle":"2023-05-16T21:07:58.599637Z","shell.execute_reply.started":"2023-05-16T21:07:58.505468Z","shell.execute_reply":"2023-05-16T21:07:58.598540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x, y in tf.data.Dataset.load(SAVE_PATH).take(1):\n    print(x.shape)\n    print(y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.601336Z","iopub.execute_input":"2023-05-16T21:07:58.601693Z","iopub.status.idle":"2023-05-16T21:07:58.662723Z","shell.execute_reply.started":"2023-05-16T21:07:58.601653Z","shell.execute_reply":"2023-05-16T21:07:58.661481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set constants and pick important landmarks\nLANDMARK_IDX = list(range(1,41))\nDS_CARDINALITY = 10\nN_SIGNS = 14","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.671413Z","iopub.execute_input":"2023-05-16T21:07:58.672336Z","iopub.status.idle":"2023-05-16T21:07:58.678245Z","shell.execute_reply.started":"2023-05-16T21:07:58.672286Z","shell.execute_reply":"2023-05-16T21:07:58.677129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.load(SAVE_PATH)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.679893Z","iopub.execute_input":"2023-05-16T21:07:58.680376Z","iopub.status.idle":"2023-05-16T21:07:58.708159Z","shell.execute_reply.started":"2023-05-16T21:07:58.680339Z","shell.execute_reply":"2023-05-16T21:07:58.706874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(ragged_batch, labels):\n    ragged_batch = tf.gather(ragged_batch, LANDMARK_IDX, axis=2)\n    ragged_batch = tf.where(tf.math.is_nan(ragged_batch), tf.zeros_like(ragged_batch), ragged_batch)\n    return tf.concat([ragged_batch[...,i] for i in range(3)],-1), labels\n\ntrain_ds = dataset.map(preprocess)\n# val_ds = dataset.take(VAL_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n# train_ds = dataset.skip(VAL_SIZE).cache().shuffle(20).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:07:58.709858Z","iopub.execute_input":"2023-05-16T21:07:58.710247Z","iopub.status.idle":"2023-05-16T21:07:59.998281Z","shell.execute_reply.started":"2023-05-16T21:07:58.710215Z","shell.execute_reply":"2023-05-16T21:07:59.997042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_ds = tf.data.Dataset.from_tensor_slices(\n    val.path.values                                              # start with a dataset of the parquet paths\n).map(\n    tf_get_features                                                   # load individual sequences\n).map(\n    set_shape                                                         # set and enforce element shape\n).apply(\n    tf.data.experimental.dense_to_ragged_batch(batch_size=BATCH_SIZE) # apply batching function\n)\n\n# load and batch the labels\ny_val_ds = tf.data.Dataset.from_tensor_slices(\n    val.sign.map(SIGN2ORD).values.reshape(-1,1)\n).batch(BATCH_SIZE)\n\n# zip the features and labels\nval_ds = tf.data.Dataset.zip((X_val_ds, y_val_ds))\n\nval_ds.prefetch(tf.data.AUTOTUNE).save(SAVE_PATH, shard_func=shard_func)\n\ncheck_throughput(SAVE_PATH)\n\ndataset = tf.data.Dataset.load(SAVE_PATH)\n\nval_ds = dataset.map(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:08:00.003878Z","iopub.execute_input":"2023-05-16T21:08:00.004819Z","iopub.status.idle":"2023-05-16T21:08:01.506894Z","shell.execute_reply.started":"2023-05-16T21:08:00.004769Z","shell.execute_reply":"2023-05-16T21:08:01.505277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:08:01.509301Z","iopub.execute_input":"2023-05-16T21:08:01.510464Z","iopub.status.idle":"2023-05-16T21:08:01.517763Z","shell.execute_reply.started":"2023-05-16T21:08:01.510411Z","shell.execute_reply":"2023-05-16T21:08:01.516503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"execution":{"iopub.status.busy":"2023-05-16T21:08:01.519299Z","iopub.execute_input":"2023-05-16T21:08:01.519671Z","iopub.status.idle":"2023-05-16T21:08:01.529964Z","shell.execute_reply.started":"2023-05-16T21:08:01.519638Z","shell.execute_reply":"2023-05-16T21:08:01.528657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### WORK STILL IN PROGRESS","metadata":{}},{"cell_type":"markdown","source":"## Thank you!","metadata":{}},{"cell_type":"markdown","source":"* Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments.\n* If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}